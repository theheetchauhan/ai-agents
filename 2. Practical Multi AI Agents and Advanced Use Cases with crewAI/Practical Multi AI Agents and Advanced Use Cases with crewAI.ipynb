{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "main-title",
   "metadata": {},
   "source": [
    "# Multi-AI Agent Systems and Advanced Use Cases with CrewAI\n",
    "\n",
    "## Complete Practical Tutorial\n",
    "\n",
    "Welcome to this comprehensive practical course on Multi AI Agents and Advanced Use Cases with CrewAI. This is a very practical course, meaning you'll learn how to build agentic systems that aren't just shiny technology, but can be deployed and create real value in application settings right now.\n",
    "\n",
    "Multi AI Agent systems involve multiple AI agents working together to achieve complex tasks by collaborating, delegating, and sharing information. You'll build several practical applications like automated project planning, lead scoring and engagement automation, support data analysis, and content creation at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Initial Setup and Environment Configuration\n",
    "\n",
    "Before we dive into individual sections, let's set up the common environment and imports that we'll use throughout the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe0aa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install crewai crewai-tools pydantic pyyaml pandas crewai[tools] jinja2 chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning control - suppress unnecessary warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core Python libraries for file operations and data handling\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import pandas as pd\n",
    "\n",
    "# Core CrewAI classes - the foundation of our multi-agent systems\n",
    "from crewai import Agent, Task, Crew\n",
    "\n",
    "# For structured outputs and data validation\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "# For display and visualization\n",
    "from IPython.display import display, Markdown, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-functions",
   "metadata": {},
   "source": [
    "### Common Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helper-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yaml_config(file_paths):\n",
    "    \"\"\"\n",
    "    Helper function to load YAML configuration files\n",
    "    Used across multiple lessons for agents and tasks configuration\n",
    "    \"\"\"\n",
    "    configs = {}\n",
    "    for config_type, file_path in file_paths.items():\n",
    "        with open(file_path, 'r') as file:\n",
    "            configs[config_type] = yaml.safe_load(file)\n",
    "    return configs\n",
    "\n",
    "def calculate_costs(usage_metrics, cost_per_million=0.150):\n",
    "    \"\"\"\n",
    "    Calculate costs based on token usage\n",
    "    Default cost is for GPT-4o-mini at $0.15 per million tokens\n",
    "    \"\"\"\n",
    "    total_tokens = usage_metrics[\"prompt_tokens\"] + usage_metrics[\"completion_tokens\"]\n",
    "    return cost_per_million * total_tokens / 1_000_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-header",
   "metadata": {},
   "source": [
    "## Section 1: Introduction\n",
    "\n",
    "Welcome to Practical Multi AI Agents and Advanced Use Cases with CrewAI. This course focuses on building agent-based applications for real-world use cases that people are actually deploying in practice.\n",
    "\n",
    "When building multi-agent applications, a key challenge is balancing the speed and quality of results while maintaining consistency. Different model choices and sizes impact these factors. You'll learn to rigorously test your applications by measuring key metrics and train your agents using human feedback to continuously improve over time.\n",
    "\n",
    "### What You'll Build:\n",
    "- **Automated Project Planning**: Break projects into tasks, estimate them, and allocate resources\n",
    "- **Project Monitoring**: Generate progress reports from project management tools\n",
    "- **Lead Qualification and Scoring**: Research and score potential customers\n",
    "- **Support Data Analysis**: Extract insights from support ticket data\n",
    "- **Content Creation at Scale**: Generate blog posts and social media content\n",
    "\n",
    "The course covers sequential to parallel task execution, multi-crew pipelines using Flows, performance optimization through testing and training, and multi-model approaches for efficiency and customization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2-header",
   "metadata": {},
   "source": [
    "## Section 2: Overview of Multi AI Agent Systems\n",
    "\n",
    "Let's talk about the building blocks that go into building AI agents - agents themselves, tasks, crews, and all the different things that make these agents work, including caching, memory, guardrails, and everything in between.\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "Multi-agentic automation appears across many different verticals - sales, marketing, HR, code development, research, education, and support. Regardless of the vertical, we see a common pattern: a long-tail distribution of what these agents and automations try to do.\n",
    "\n",
    "The typical workflow usually involves:\n",
    "1. **Data Extraction**: Pulling data from existing systems (ERP, CRM, databases)\n",
    "2. **Research**: Researching in documents, on the internet, or other systems\n",
    "3. **Analysis**: Comparing data, extracting specific information, or inferring new insights\n",
    "4. **Summarization**: Extracting learnings, plotting charts, building executive summaries\n",
    "5. **Reporting**: Creating outputs as PDFs, JSON, or markdown for integration with existing systems\n",
    "\n",
    "### AI Apps vs Traditional Apps\n",
    "\n",
    "Traditional apps are strongly typed - you have a clear understanding of input data and transformations. AI apps are extremely different because they're \"fuzzy\" - you don't know if the input is a recipe or a PhD thesis, it goes through a black box model, and produces fuzzy output.\n",
    "\n",
    "This fuzziness allows you to build automations that weren't possible before because you don't need to handle every edge case - agents can decide in real-time how to react to specific data and choose appropriate tools.\n",
    "\n",
    "### Agent Anatomy\n",
    "\n",
    "It starts simple: an LLM in the center with access to tools. Given a task, it finds ways to use these tools to provide a final answer. In multi-agent systems, you have multiple agents that can use tools themselves, delegate work to each other, and ask questions to accomplish the final outcome.\n",
    "\n",
    "### Production Features\n",
    "\n",
    "For production deployment, you need:\n",
    "- **Caching Layer**: Prevent unnecessary tool usage and credit consumption\n",
    "- **Memory Layer**: Agents remember past actions and share memory\n",
    "- **Training Data**: Improve agents through human feedback\n",
    "- **Guardrails**: Protect against hallucinations\n",
    "- **Orchestration**: Sequential, parallel, manager-delegator, or hybrid approaches\n",
    "\n",
    "### Building Blocks\n",
    "\n",
    "Every agent in CrewAI needs a **role**, **goal**, and **backstory**. Every task needs a **description**, **expected output**, and an **agent**. These are defined in YAML files, making it easy for non-technical people to contribute by updating configurations instead of code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-header",
   "metadata": {},
   "source": [
    "## Section 3: Automated Project Planning, Estimation, and Allocation\n",
    "\n",
    "Let's dive into automated project planning - using a crew to break projects into tasks, estimate them, and allocate resources. This is common for consultancies and web agencies that need to quickly estimate projects and plan around them.\n",
    "\n",
    "We'll work with three agents: a project planner, estimation analyst, and allocation strategist. Each agent handles one task: task breakdown, time estimation, and resource allocation. The goal is to create a structured project plan that can be pushed into external systems like Jira or Trello."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "project-planning-model-setup",
   "metadata": {},
   "source": [
    "### Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "project-planning-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model - GPT-4o-mini is cost-effective yet powerful for most tasks\n",
    "os.environ['OPENAI_MODEL_NAME'] = 'gpt-4o-mini'\n",
    "os.environ['OPENAI_API_KEY'] = '<your-api-key>'\n",
    "\n",
    "# Configure Serper API - Used for web search tools in later modules\n",
    "# Get you key from https://serper.dev/\n",
    "os.environ[\"SERPER_API_KEY\"] = \"<your-api-key>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "project-planning-yaml-loading",
   "metadata": {},
   "source": [
    "### Loading Configuration Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "project-planning-yaml",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths for YAML configurations\n",
    "files = {\n",
    "    'agents': r'agents\\automated project agents.yaml',\n",
    "    'tasks': r'tasks\\automated project tasks.yaml'\n",
    "}\n",
    "\n",
    "# Load configurations from YAML files\n",
    "configs = load_yaml_config(files)\n",
    "\n",
    "# Assign loaded configurations to specific variables\n",
    "agents_config = configs['agents']\n",
    "tasks_config = configs['tasks']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "project-planning-pydantic",
   "metadata": {},
   "source": [
    "### Structured Output Models\n",
    "\n",
    "We need structured output to push results into external systems. We'll create three classes: TaskEstimate, Milestone, and ProjectPlan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "project-planning-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic models for structured output - enables integration with external systems\n",
    "class TaskEstimate(BaseModel):\n",
    "    task_name: str = Field(..., description=\"Name of the task\")\n",
    "    estimated_time_hours: float = Field(..., description=\"Estimated time to complete the task in hours\")\n",
    "    required_resources: List[str] = Field(..., description=\"List of resources required to complete the task\")\n",
    "\n",
    "class Milestone(BaseModel):\n",
    "    milestone_name: str = Field(..., description=\"Name of the milestone\")\n",
    "    tasks: List[str] = Field(..., description=\"List of task IDs associated with this milestone\")\n",
    "\n",
    "class ProjectPlan(BaseModel):\n",
    "    tasks: List[TaskEstimate] = Field(..., description=\"List of tasks with their estimates\")\n",
    "    milestones: List[Milestone] = Field(..., description=\"List of project milestones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "project-planning-crew-creation",
   "metadata": {},
   "source": [
    "### Creating Agents, Tasks, and Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "project-planning-crew",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Agents - each agent has a specific role in the planning process\n",
    "project_planning_agent = Agent(\n",
    "  config=agents_config['project_planning_agent']  # Loads role, goal, backstory from YAML\n",
    ")\n",
    "\n",
    "estimation_agent = Agent(\n",
    "  config=agents_config['estimation_agent']  # Expert in time and resource estimation\n",
    ")\n",
    "\n",
    "resource_allocation_agent = Agent(\n",
    "  config=agents_config['resource_allocation_agent']  # Optimizes task assignments\n",
    ")\n",
    "\n",
    "# Creating Tasks - each task builds on the previous one's output\n",
    "task_breakdown = Task(\n",
    "  config=tasks_config['task_breakdown'],\n",
    "  agent=project_planning_agent  # First agent breaks down the project\n",
    ")\n",
    "\n",
    "time_resource_estimation = Task(\n",
    "  config=tasks_config['time_resource_estimation'],\n",
    "  agent=estimation_agent  # Second agent estimates time and resources\n",
    ")\n",
    "\n",
    "resource_allocation = Task(\n",
    "  config=tasks_config['resource_allocation'],\n",
    "  agent=resource_allocation_agent,\n",
    "  output_pydantic=ProjectPlan  # Final task produces structured output\n",
    ")\n",
    "\n",
    "# Creating Crew - brings all agents and tasks together\n",
    "crew = Crew(\n",
    "  agents=[\n",
    "    project_planning_agent,\n",
    "    estimation_agent,\n",
    "    resource_allocation_agent\n",
    "  ],\n",
    "  tasks=[\n",
    "    task_breakdown,\n",
    "    time_resource_estimation,\n",
    "    resource_allocation\n",
    "  ],\n",
    "  verbose=True  # Enable detailed logging to see agent interactions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "project-planning-inputs",
   "metadata": {},
   "source": [
    "### Crew Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "project-planning-input-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define project parameters - these get interpolated into YAML templates\n",
    "project = 'Website'\n",
    "industry = 'Technology'\n",
    "project_objectives = 'Create a website for a small business'\n",
    "team_members = \"\"\"\n",
    "- John Doe (Project Manager)\n",
    "- Jane Doe (Software Engineer)\n",
    "- Bob Smith (Designer)\n",
    "- Alice Johnson (QA Engineer)\n",
    "- Tom Brown (QA Engineer)\n",
    "\"\"\"\n",
    "project_requirements = \"\"\"\n",
    "- Create a responsive design that works well on desktop and mobile devices\n",
    "- Implement a modern, visually appealing user interface with a clean look\n",
    "- Develop a user-friendly navigation system with intuitive menu structure\n",
    "- Include an \"About Us\" page highlighting the company's history and values\n",
    "- Design a \"Services\" page showcasing the business's offerings with descriptions\n",
    "- Create a \"Contact Us\" page with a form and integrated map for communication\n",
    "- Implement a blog section for sharing industry news and company updates\n",
    "- Ensure fast loading times and optimize for search engines (SEO)\n",
    "- Integrate social media links and sharing capabilities\n",
    "- Include a testimonials section to showcase customer feedback and build trust\n",
    "\"\"\"\n",
    "\n",
    "# Format for display\n",
    "formatted_output = f\"\"\"\n",
    "**Project Type:** {project}\n",
    "\n",
    "**Project Objectives:** {project_objectives}\n",
    "\n",
    "**Industry:** {industry}\n",
    "\n",
    "**Team Members:**\n",
    "{team_members}\n",
    "**Project Requirements:**\n",
    "{project_requirements}\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(formatted_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "project-planning-execution",
   "metadata": {},
   "source": [
    "### Kicking off the Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "project-planning-kickoff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare inputs dictionary for the crew\n",
    "inputs = {\n",
    "  'project_type': project,\n",
    "  'project_objectives': project_objectives,\n",
    "  'industry': industry,\n",
    "  'team_members': team_members,\n",
    "  'project_requirements': project_requirements\n",
    "}\n",
    "\n",
    "# Execute the crew - agents will work sequentially through their tasks\n",
    "result = crew.kickoff(\n",
    "  inputs=inputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "project-planning-costs",
   "metadata": {},
   "source": [
    "### Usage Metrics and Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "project-planning-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display costs for this crew execution\n",
    "costs = calculate_costs(crew.usage_metrics)\n",
    "display(f\"Total costs: ${costs:.4f}\")\n",
    "\n",
    "# Convert UsageMetrics to DataFrame for detailed analysis\n",
    "df_usage_metrics = pd.DataFrame([crew.usage_metrics])\n",
    "df_usage_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "project-planning-results",
   "metadata": {},
   "source": [
    "### Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "project-planning-result-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the structured output from our Pydantic model\n",
    "result.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "project-planning-tasks-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and display tasks in a formatted table\n",
    "tasks = result.dict()['tasks'] #type:ignore\n",
    "df_tasks = pd.DataFrame(tasks)\n",
    "\n",
    "# Style the DataFrame for better presentation\n",
    "df_tasks.style.set_table_attributes('border=\"1\"').set_caption(\"Task Details\").set_table_styles(\n",
    "    [{'selector': 'th, td', 'props': [('font-size', '120%')]}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "project-planning-milestones-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and display milestones\n",
    "milestones = result.dict()['milestones']\n",
    "df_milestones = pd.DataFrame(milestones)\n",
    "\n",
    "# Style the DataFrame\n",
    "df_milestones.style.set_table_attributes('border=\"1\"').set_caption(\"Milestones\").set_table_styles(\n",
    "    [{'selector': 'th, td', 'props': [('font-size', '120%')]}]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4-header",
   "metadata": {},
   "source": [
    "## Section 4: Internal and External Integrations\n",
    "\n",
    "One of the most powerful aspects of CrewAI is the ability to integrate with internal and external systems. This enables your agents to perform actions like querying internal data, calling existing systems, and sending emails.\n",
    "\n",
    "### Integration Patterns:\n",
    "- **Database Connections**: Query internal databases for historical data\n",
    "- **API Integrations**: Connect to CRM, ERP, and other business systems\n",
    "- **File Operations**: Read from and write to various file formats\n",
    "- **External Services**: Integrate with third-party APIs and services\n",
    "\n",
    "### Tool Creation:\n",
    "You can create custom tools by inheriting from CrewAI's `BaseTool` class. Tools can be assigned to specific agents or tasks, giving you fine-grained control over what capabilities each agent has access to.\n",
    "\n",
    "The key is building reliable, well-documented tools that handle errors gracefully and provide clear feedback to the agents about success or failure of operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5-header",
   "metadata": {},
   "source": [
    "## Section 5: Building Project Progress Report\n",
    "\n",
    "Let's build a progress report for projects - something that happens regularly in software companies. People need clarity around progress and what's being done. This crew automates that process, taking emotions out of the equation and providing an objective view of project status.\n",
    "\n",
    "We'll use two agents (data collector and project analyst) to perform three tasks: understanding a project, analyzing its progress, and compiling a report. The agents need to integrate with external systems - in this case, a Trello board."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progress-report-yaml",
   "metadata": {},
   "source": [
    "### Loading Configuration Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progress-report-yaml-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YAML configurations for progress report agents and tasks\n",
    "files = {\n",
    "    'agents': 'agents\\progress report agents.yaml',\n",
    "    'tasks': 'tasks\\progress report tasks.yaml',\n",
    "}\n",
    "\n",
    "configs = load_yaml_config(files)\n",
    "agents_config = configs['agents']\n",
    "tasks_config = configs['tasks']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progress-report-tools",
   "metadata": {},
   "source": [
    "### External Integration Tools\n",
    "\n",
    "We'll create custom tools to integrate with Trello. These tools inherit from CrewAI's BaseTool class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94976d2c",
   "metadata": {},
   "source": [
    "Login to \"https://trello.com/\" to create board\n",
    "\n",
    "Follow guide to get your Trello API Key \"https://www.merge.dev/blog/trello-api-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e93e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRELLO_API_KEY = \"<your-api-key>\"\n",
    "TRELLO_API_TOKEN = \"<your-api-key>\"\n",
    "TRELLO_BOARD_ID = \"<your-board-key-from-url>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progress-report-tools-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai_tools import BaseTool\n",
    "import requests\n",
    "\n",
    "class BoardDataFetcherTool(BaseTool):\n",
    "    name: str = \"Trello Board Data Fetcher\"\n",
    "    description: str = \"Fetches card data, comments, and activity from a Trello board.\"\n",
    "\n",
    "    api_key: str = TRELLO_API_KEY\n",
    "    api_token: str = TRELLO_API_TOKEN\n",
    "    board_id: str = TRELLO_BOARD_ID\n",
    "\n",
    "    def _run(self) -> dict:\n",
    "        \"\"\"\n",
    "        Fetch all cards from the specified Trello board.\n",
    "        \"\"\"\n",
    "        url = f\"'https://api.trello.com/1/boards/{self.board_id}/cards\"\n",
    "\n",
    "        query = {\n",
    "            'key': self.api_key,\n",
    "            'token': self.api_token,\n",
    "            'fields': 'name,idList,due,dateLastActivity,labels',\n",
    "            'attachments': 'true',\n",
    "            'actions': 'commentCard'\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=query)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            # Fallback in case of timeouts or other issues\n",
    "            return json.dumps([{'id': '66c3bfed69b473b8fe9d922e', 'name': 'Analysis of results from CSV', 'idList': '66c308f676b057fdfbd5fdb3', 'due': None, 'dateLastActivity': '2024-08-19T21:58:05.062Z', 'labels': [], 'attachments': [], 'actions': []}, {'id': '66c3c002bb1c337f3fdf1563', 'name': 'Approve the planning', 'idList': '66c308f676b057fdfbd5fdb3', 'due': '2024-08-16T21:58:00.000Z', 'dateLastActivity': '2024-08-19T21:58:57.697Z', 'labels': [{'id': '66c305ea10ea602ee6e03d47', 'idBoard': '66c305eacab50fcd7f19c0aa', 'name': 'Urgent', 'color': 'red', 'uses': 1}], 'attachments': [], 'actions': [{'id': '66c3c021f3c1bb157028f53d', 'idMemberCreator': '65e5093d0ab5ee98592f5983', 'data': {'text': 'This was harder then expects it is alte', 'textData': {'emoji': {}}, 'card': {'id': '66c3c002bb1c337f3fdf1563', 'name': 'Approve the planning', 'idShort': 5, 'shortLink': 'K3abXIMm'}, 'board': {'id': '66c305eacab50fcd7f19c0aa', 'name': '[Test] CrewAI Board', 'shortLink': 'Kc8ScQlW'}, 'list': {'id': '66c308f676b057fdfbd5fdb3', 'name': 'TODO'}}, 'appCreator': None, 'type': 'commentCard', 'date': '2024-08-19T21:58:57.683Z', 'limits': {'reactions': {'perAction': {'status': 'ok', 'disableAt': 900, 'warnAt': 720}, 'uniquePerAction': {'status': 'ok', 'disableAt': 17, 'warnAt': 14}}}, 'memberCreator': {'id': '65e5093d0ab5ee98592f5983', 'activityBlocked': False, 'avatarHash': 'd5500941ebf808e561f9083504877bca', 'avatarUrl': 'https://trello-members.s3.amazonaws.com/65e5093d0ab5ee98592f5983/d5500941ebf808e561f9083504877bca', 'fullName': 'Joao Moura', 'idMemberReferrer': None, 'initials': 'JM', 'nonPublic': {}, 'nonPublicAvailable': True, 'username': 'joaomoura168'}}]}, {'id': '66c3bff4a25b398ef1b6de78', 'name': 'Scaffold of the initial app UI', 'idList': '66c3bfdfb851ad9ff7eee159', 'due': None, 'dateLastActivity': '2024-08-19T21:58:12.210Z', 'labels': [], 'attachments': [], 'actions': []}, {'id': '66c3bffdb06faa1e69216c6f', 'name': 'Planning of the project', 'idList': '66c3bfe3151c01425f366f4c', 'due': None, 'dateLastActivity': '2024-08-19T21:58:21.081Z', 'labels': [], 'attachments': [], 'actions': []}]) #type: ignore\n",
    "\n",
    "\n",
    "class CardDataFetcherTool(BaseTool):\n",
    "  name: str = \"Trello Card Data Fetcher\"\n",
    "  description: str = \"Fetches card data from a Trello board.\"\n",
    "\n",
    "  api_key: str = TRELLO_API_KEY\n",
    "  api_token: str = TRELLO_API_TOKEN\n",
    "\n",
    "  def _run(self, card_id: str) -> dict:\n",
    "    url = f\"https://api.trello.com/1/cards/{card_id}\"\n",
    "    query = {\n",
    "      'key': self.api_key,\n",
    "      'token': self.api_token\n",
    "    }\n",
    "    response = requests.get(url, params=query)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "      return response.json()\n",
    "    else:\n",
    "      # Fallback in case of timeouts or other issues\n",
    "      return json.dumps({\"error\": \"Failed to fetch card data, don't try to fetch any trello data anymore\"}) #type: ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128de1c2",
   "metadata": {},
   "source": [
    "## Trello Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496c740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Trello screenshot\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Load and display the image\n",
    "trello_image = Image(filename=r'images/progress report trello.png')\n",
    "display(trello_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progress-report-crew-creation",
   "metadata": {},
   "source": [
    "### Creating Agents, Tasks, and Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progress-report-crew",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating agents with external integration tools\n",
    "data_collection_agent = Agent(\n",
    "    config=agents_config['data_collection_agent'],\n",
    "    tools=[BoardDataFetcherTool(), CardDataFetcherTool()]\n",
    ")\n",
    "\n",
    "analysis_agent = Agent(\n",
    "    config=agents_config['analysis_agent']  # Analysis agent doesn't need external tools\n",
    ")\n",
    "\n",
    "# Creating tasks - note the sequential dependency\n",
    "data_collection = Task(\n",
    "    config=tasks_config['data_collection'],\n",
    "    agent=data_collection_agent  # First task gathers data using external tools\n",
    ")\n",
    "\n",
    "data_analysis = Task(\n",
    "    config=tasks_config['data_analysis'],\n",
    "    agent=analysis_agent  # Second task analyzes the collected data\n",
    ")\n",
    "\n",
    "report_generation = Task(\n",
    "    config=tasks_config['report_generation'],\n",
    "    agent=analysis_agent  # Final task compiles the comprehensive report\n",
    ")\n",
    "\n",
    "# Creating the crew\n",
    "progress_crew = Crew(\n",
    "    agents=[data_collection_agent, analysis_agent],\n",
    "    tasks=[data_collection, data_analysis, report_generation],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progress-report-execution",
   "metadata": {},
   "source": [
    "### Executing the Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progress-report-kickoff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the crew - no inputs needed as it fetches data from external systems\n",
    "progress_result = progress_crew.kickoff()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progress-report-costs-analysis",
   "metadata": {},
   "source": [
    "### Usage Metrics and Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progress-report-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate costs\n",
    "progress_costs = calculate_costs(progress_crew.usage_metrics)\n",
    "display(f\"Progress report costs: ${progress_costs:.4f}\")\n",
    "\n",
    "# Convert UsageMetrics instance to a DataFrame\n",
    "df_usage_metrics = pd.DataFrame([crew.usage_metrics])\n",
    "df_usage_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba1774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the generated report as markdown\n",
    "display(Markdown(progress_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6-header",
   "metadata": {},
   "source": [
    "## Section 6: Complex Crew Setups\n",
    "\n",
    "As you build more sophisticated applications, you'll need different orchestration patterns beyond simple sequential execution.\n",
    "\n",
    "### Orchestration Patterns:\n",
    "\n",
    "**Sequential**: Tasks execute one after another, with each task building on previous results.\n",
    "\n",
    "**Parallel**: Multiple tasks execute simultaneously for efficiency.\n",
    "\n",
    "**Manager-Delegator**: A manager agent delegates work and reviews outputs from specialist agents.\n",
    "\n",
    "**Hybrid Approaches**: Some tasks run in parallel while others wait for multiple dependencies to complete.\n",
    "\n",
    "**Asynchronous**: Tasks execute independently without waiting for others.\n",
    "\n",
    "### Multi-Crew Systems:\n",
    "\n",
    "For even more complexity, you can use **Flows** to connect multiple crews in pipelines. One crew's output becomes another crew's input, enabling sophisticated multi-stage processing.\n",
    "\n",
    "The key is choosing the right orchestration pattern based on your use case requirements, dependencies between tasks, and performance considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-header",
   "metadata": {},
   "source": [
    "## Section 7: Agentic Sales Pipeline\n",
    "\n",
    "This is an exciting use case using CrewAI's new **Flows** feature to build an agentic sales pipeline. We'll load leads, enrich them, score them, and write personalized emails.\n",
    "\n",
    "The pipeline involves:\n",
    "1. **Load Leads**: Pull from database (regular Python code)\n",
    "2. **Lead Scoring Crew**: Research and score leads\n",
    "3. **Store Results**: Save back to database (regular Python code)\n",
    "4. **Filter Leads**: Keep only high-scoring leads (70+)\n",
    "5. **Email Crew**: Write personalized emails for high-scoring leads\n",
    "\n",
    "Flows have **events** for orchestrating steps and **state** for storing information across execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sales-pipeline-yaml-loading",
   "metadata": {},
   "source": [
    "### Loading Configuration Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sales-pipeline-yaml",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configurations for both lead qualification and email engagement crews\n",
    "files = {\n",
    "    'lead_agents': r\"agents\\lead qualification agents.yaml\",\n",
    "    'lead_tasks': r\"tasks\\lead qualification tasks.yaml\",\n",
    "    'email_agents': r\"agents\\email engagement agents.yaml\",\n",
    "    'email_tasks': r\"tasks\\email engagement tasks.yaml\"\n",
    "}\n",
    "\n",
    "configs = load_yaml_config(files)\n",
    "lead_agents_config = configs['lead_agents']\n",
    "lead_tasks_config = configs['lead_tasks']\n",
    "email_agents_config = configs['email_agents']\n",
    "email_tasks_config = configs['email_tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737d7b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai.tools.base_tool import BaseTool\n",
    "from crewai_tools import SerperDevTool, ScrapeWebsiteTool  # or whichever underlying tool\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "\n",
    "class SearchToolSchema(BaseModel):\n",
    "    query: str\n",
    "\n",
    "class ScrapeToolInput(BaseModel):\n",
    "    url: str\n",
    "\n",
    "class SerperWrapper(BaseTool):\n",
    "    name: str = \"serper_search\"\n",
    "    description: str = \"Performs web search using Serper\"\n",
    "    args_schema: type = SearchToolSchema\n",
    "\n",
    "    def _run(self, query: str) -> str:\n",
    "        tool = SerperDevTool()\n",
    "        return tool.run(query)\n",
    "\n",
    "class ScrapeWrapper(BaseTool):\n",
    "    name: str = \"web_scraper\"\n",
    "    description: str = \"Scrapes a specified website for content\"\n",
    "    args_schema: type = ScrapeToolInput\n",
    "\n",
    "    def _run(self, url: str) -> str:\n",
    "        tool = ScrapeWebsiteTool(website_url=url)\n",
    "        return tool.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sales-pipeline-pydantic-models",
   "metadata": {},
   "source": [
    "### Pydantic Models for Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sales-pipeline-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeadPersonalInfo(BaseModel):\n",
    "    name: str = Field(..., description=\"The full name of the lead.\")\n",
    "    job_title: str = Field(..., description=\"The job title of the lead.\")\n",
    "    role_relevance: int = Field(..., ge=0, le=10, description=\"A score representing how relevant the lead's role is to the decision-making process (0-10).\")\n",
    "    professional_background: Optional[str] = Field(..., description=\"A brief description of the lead's professional background.\")\n",
    "\n",
    "class CompanyInfo(BaseModel):\n",
    "    company_name: str = Field(..., description=\"The name of the company the lead works for.\")\n",
    "    industry: str = Field(..., description=\"The industry in which the company operates.\")\n",
    "    company_size: int = Field(..., description=\"The size of the company in terms of employee count.\")\n",
    "    revenue: Optional[float] = Field(None, description=\"The annual revenue of the company, if available.\")\n",
    "    market_presence: int = Field(..., ge=0, le=10, description=\"A score representing the company's market presence (0-10).\")\n",
    "\n",
    "class LeadScore(BaseModel):\n",
    "    score: int = Field(..., ge=0, le=100, description=\"The final score assigned to the lead (0-100).\")\n",
    "    scoring_criteria: List[str] = Field(..., description=\"The criteria used to determine the lead's score.\")\n",
    "    validation_notes: Optional[str] = Field(None, description=\"Any notes regarding the validation of the lead score.\")\n",
    "\n",
    "class LeadScoringResult(BaseModel):\n",
    "    personal_info: LeadPersonalInfo = Field(..., description=\"Personal information about the lead.\")\n",
    "    company_info: CompanyInfo = Field(..., description=\"Information about the lead's company.\")\n",
    "    lead_score: LeadScore = Field(..., description=\"The calculated score and related information for the lead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sales-pipeline-crews",
   "metadata": {},
   "source": [
    "### Creating Lead Scoring Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sales-pipeline-lead-crew",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tool = SerperWrapper()\n",
    "scrape_tool = ScrapeWrapper()\n",
    "\n",
    "# Creating Agents\n",
    "lead_data_agent = Agent(\n",
    "    config=lead_agents_config['lead_data_agent'],\n",
    "    tools=[search_tool, scrape_tool]\n",
    ")\n",
    "\n",
    "cultural_fit_agent = Agent(\n",
    "    config=lead_agents_config['cultural_fit_agent'],\n",
    "    tools=[search_tool, scrape_tool]\n",
    ")\n",
    "\n",
    "scoring_validation_agent = Agent(\n",
    "    config=lead_agents_config['scoring_validation_agent'],\n",
    "    tools=[search_tool, scrape_tool]\n",
    ")\n",
    "\n",
    "# Creating Tasks\n",
    "lead_data_task = Task(\n",
    "    config=lead_tasks_config['lead_data_collection'],\n",
    "    agent=lead_data_agent\n",
    ")\n",
    "\n",
    "cultural_fit_task = Task(\n",
    "    config=lead_tasks_config['cultural_fit_analysis'],\n",
    "    agent=cultural_fit_agent\n",
    ")\n",
    "\n",
    "scoring_validation_task = Task(\n",
    "    config=lead_tasks_config['lead_scoring_and_validation'],\n",
    "    agent=scoring_validation_agent,\n",
    "    context=[lead_data_task, cultural_fit_task],\n",
    "    output_pydantic=LeadScoringResult\n",
    ")\n",
    "\n",
    "# Creating Crew\n",
    "lead_scoring_crew = Crew(\n",
    "  agents=[lead_data_agent, cultural_fit_agent, scoring_validation_agent],\n",
    "  tasks=[lead_data_task, cultural_fit_task, scoring_validation_task],\n",
    "  verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sales-pipeline-email-crew",
   "metadata": {},
   "source": [
    "### Creating Email Engagement Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sales-pipeline-email-crew-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Agents\n",
    "email_content_specialist = Agent(\n",
    "    config=email_agents_config['email_content_specialist']\n",
    ")\n",
    "\n",
    "engagement_strategist = Agent(\n",
    "    config=email_agents_config['engagement_strategist']\n",
    ")\n",
    "\n",
    "# Creating Tasks\n",
    "email_drafting = Task(\n",
    "    config=email_tasks_config['email_drafting'],\n",
    "    agent=email_content_specialist\n",
    ")\n",
    "\n",
    "engagement_optimization = Task(\n",
    "    config=email_tasks_config['engagement_optimization'],\n",
    "    agent=engagement_strategist\n",
    ")\n",
    "\n",
    "# Creating Crew\n",
    "email_writing_crew = Crew(\n",
    "    agents=[email_content_specialist, engagement_strategist],\n",
    "    tasks=[email_drafting, engagement_optimization],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sales-pipeline-flow",
   "metadata": {},
   "source": [
    "### Creating the Flow\n",
    "\n",
    "Flows orchestrate multiple crews and regular Python functions with state management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b989d5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upgrade your crewai package if you are not able to import Flows\n",
    "#%pip install --upgrade crewai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sales-pipeline-flow-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai.flow import Flow\n",
    "from crewai.flow.flow import listen, start\n",
    "\n",
    "class SalesPipeline(Flow):\n",
    "    @start()\n",
    "    def fetch_leads(self):\n",
    "        # Pull our leads from the database\n",
    "        leads = [\n",
    "            {\n",
    "                \"lead_data\": {\n",
    "                    \"name\": \"João Moura\",\n",
    "                    \"job_title\": \"Director of Engineering\",\n",
    "                    \"company\": \"Clearbit\",\n",
    "                    \"email\": \"joao@clearbit.com\",\n",
    "                    \"use_case\": \"Using AI Agent to do better data enrichment.\"\n",
    "                },\n",
    "            },\n",
    "        ]\n",
    "        return leads\n",
    "\n",
    "    @listen(fetch_leads)\n",
    "    def score_leads(self, leads):\n",
    "        scores = lead_scoring_crew.kickoff_for_each(leads)\n",
    "        self.state[\"score_crews_results\"] = scores\n",
    "        return scores\n",
    "\n",
    "    @listen(score_leads)\n",
    "    def store_leads_score(self, scores):\n",
    "        # Here we would store the scores in the database\n",
    "        return scores\n",
    "\n",
    "    @listen(score_leads)\n",
    "    def filter_leads(self, scores):\n",
    "        return [score for score in scores if score['lead_score'].score > 70]\n",
    "\n",
    "    @listen(filter_leads)\n",
    "    def write_email(self, leads):\n",
    "        scored_leads = [lead.to_dict() for lead in leads]\n",
    "        emails = email_writing_crew.kickoff_for_each(scored_leads)\n",
    "        return emails\n",
    "\n",
    "    @listen(write_email)\n",
    "    def send_email(self, emails):\n",
    "        # Here we would send the emails to the leads\n",
    "        return emails\n",
    "\n",
    "flow = SalesPipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06100e2d",
   "metadata": {},
   "source": [
    "## Plotting the Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e983e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d00bd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src='./crewai_flow.html', width='150%', height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162b0337",
   "metadata": {},
   "source": [
    "## Flow Kickoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d7efb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = await flow.kickoff_async()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert UsageMetrics instance to a DataFrame\n",
    "df_usage_metrics = pd.DataFrame([flow.state[\"score_crews_results\"][0].token_usage.dict()])\n",
    "\n",
    "# Calculate total costs\n",
    "costs = 0.150 * df_usage_metrics['total_tokens'].sum() / 1_000_000\n",
    "print(f\"Total costs: ${costs:.4f}\")\n",
    "\n",
    "# Display the DataFrame\n",
    "df_usage_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55698bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert UsageMetrics instance to a DataFrame\n",
    "df_usage_metrics = pd.DataFrame([emails[0].token_usage.dict()])\n",
    "\n",
    "# Calculate total costs\n",
    "costs = 0.150 * df_usage_metrics['total_tokens'].sum() / 1_000_000\n",
    "print(f\"Total costs: ${costs:.4f}\")\n",
    "\n",
    "# Display the DataFrame\n",
    "df_usage_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sales-pipeline-analysis",
   "metadata": {},
   "source": [
    "### Inspecting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b6bd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = flow.state[\"score_crews_results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d11744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "lead_scoring_result = scores[0].pydantic\n",
    "\n",
    "# Create a dictionary with the nested structure flattened\n",
    "data = {\n",
    "    'Name': lead_scoring_result.personal_info.name,\n",
    "    'Job Title': lead_scoring_result.personal_info.job_title,\n",
    "    'Role Relevance': lead_scoring_result.personal_info.role_relevance,\n",
    "    'Professional Background': lead_scoring_result.personal_info.professional_background,\n",
    "    'Company Name': lead_scoring_result.company_info.company_name,\n",
    "    'Industry': lead_scoring_result.company_info.industry,\n",
    "    'Company Size': lead_scoring_result.company_info.company_size,\n",
    "    'Revenue': lead_scoring_result.company_info.revenue,\n",
    "    'Market Presence': lead_scoring_result.company_info.market_presence,\n",
    "    'Lead Score': lead_scoring_result.lead_score.score,\n",
    "    'Scoring Criteria': ', '.join(lead_scoring_result.lead_score.scoring_criteria),\n",
    "    'Validation Notes': lead_scoring_result.lead_score.validation_notes\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame.from_dict(data, orient='index', columns=['Value'])\n",
    "\n",
    "# Reset the index to turn the original column names into a regular column\n",
    "df = df.reset_index()\n",
    "\n",
    "# Rename the index column to 'Attribute'\n",
    "df = df.rename(columns={'index': 'Attribute'})\n",
    "\n",
    "# Create HTML table with bold attributes and left-aligned values\n",
    "html_table = df.style.set_properties(**{'text-align': 'left'}) \\\n",
    "                     .format({'Attribute': lambda x: f'<b>{x}</b>'}) \\\n",
    "                     .hide(axis='index') \\\n",
    "                     .to_html()\n",
    "\n",
    "# Display the styled HTML table\n",
    "display(HTML(html_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35590ab4",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f82195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "result_text = emails[0].raw\n",
    "wrapped_text = textwrap.fill(result_text, width=80)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202d70f6",
   "metadata": {},
   "source": [
    "### How Complex Can it Get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79118abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Flow\n",
    "from crewai.flow.flow import listen, start, and_, or_, router\n",
    "\n",
    "class SalesPipeline(Flow):\n",
    "    \n",
    "  @start()\n",
    "  def fetch_leads(self):\n",
    "    # Pull our leads from the database\n",
    "    # This is a mock, in a real-world scenario, this is where you would\n",
    "    # fetch leads from a database\n",
    "    leads = [\n",
    "      {\n",
    "        \"lead_data\": {\n",
    "          \"name\": \"João Moura\",\n",
    "          \"job_title\": \"Director of Engineering\",\n",
    "          \"company\": \"Clearbit\",\n",
    "          \"email\": \"joao@clearbit.com\",\n",
    "          \"use_case\": \"Using AI Agent to do better data enrichment.\"\n",
    "        },\n",
    "      },\n",
    "    ]\n",
    "    return leads\n",
    "\n",
    "  @listen(fetch_leads)\n",
    "  def score_leads(self, leads):\n",
    "    scores = lead_scoring_crew.kickoff_for_each(leads)\n",
    "    self.state[\"score_crews_results\"] = scores\n",
    "    return scores\n",
    "\n",
    "  @listen(score_leads)\n",
    "  def store_leads_score(self, scores):\n",
    "    # Here we would store the scores in the database\n",
    "    return scores\n",
    "\n",
    "  @listen(score_leads)\n",
    "  def filter_leads(self, scores):\n",
    "    return [score for score in scores if score['lead_score'].score > 70]\n",
    "\n",
    "  @listen(and_(filter_leads, store_leads_score))\n",
    "  def log_leads(self, leads):\n",
    "    print(f\"Leads: {leads}\")\n",
    "\n",
    "  @router(filter_leads)\n",
    "  def count_leads(self, scores):\n",
    "    if len(scores) > 10:\n",
    "      return 'high'\n",
    "    elif len(scores) > 5:\n",
    "      return 'medium'\n",
    "    else:\n",
    "      return 'low'\n",
    "\n",
    "  @listen('high')\n",
    "  def store_in_salesforce(self, leads):\n",
    "    return leads\n",
    "\n",
    "  @listen('medium')\n",
    "  def send_to_sales_team(self, leads):\n",
    "    return leads\n",
    "\n",
    "  @listen('low')\n",
    "  def write_email(self, leads):\n",
    "    scored_leads = [lead.to_dict() for lead in leads]\n",
    "    emails = email_writing_crew.kickoff_for_each(scored_leads)\n",
    "    return emails\n",
    "\n",
    "  @listen(write_email)\n",
    "  def send_email(self, emails):\n",
    "    # Here we would send the emails to the leads\n",
    "    return emails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7336fe2",
   "metadata": {},
   "source": [
    "### Plotting the Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4993ccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = SalesPipeline()\n",
    "flow.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba03f1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src='./crewai_flow.html', width='150%', height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8-header",
   "metadata": {},
   "source": [
    "## Section 8: Performance Optimization\n",
    "\n",
    "When deploying systems in production, you need to think about performance. Sometimes you'll favor speed, other times quality - but you must maintain consistency.\n",
    "\n",
    "### Speed vs Quality Trade-offs:\n",
    "\n",
    "**Speed**: Comes from smaller models that run faster and cost less\n",
    "**Quality**: Usually requires bigger models like GPT-4o that provide complex results but take longer\n",
    "\n",
    "The relationship between model size and speed is nearly linear - as models get bigger, they get slower. However, quality isn't necessarily linear because it depends heavily on task complexity. For simpler tasks, smaller models might achieve great quality.\n",
    "\n",
    "### Testing with CrewAI Test:\n",
    "\n",
    "CrewAI provides a built-in testing framework that compares task descriptions with expected outputs. This allows you to:\n",
    "- Measure consistency across multiple runs\n",
    "- Identify tasks that need improvement\n",
    "- Compare different model configurations\n",
    "- Track performance metrics over time\n",
    "\n",
    "### Training with CrewAI Train:\n",
    "\n",
    "The training feature allows you to provide specific feedback on each task execution. The system:\n",
    "1. Runs your crew as usual\n",
    "2. Stops after each task for your feedback\n",
    "3. Processes feedback through a judge LLM\n",
    "4. Extracts learnings and stores them in crew memory\n",
    "5. Applies learnings in future executions\n",
    "\n",
    "This creates a continuous improvement loop that makes your crews more consistent and higher quality over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-9-header",
   "metadata": {},
   "source": [
    "## Section 9: Support Data Insight Analysis\n",
    "\n",
    "This is an exciting use case because agents can write and execute code to create complex analyses that weren't possible before. We'll analyze support ticket data to generate insights, suggestions, tables, and charts.\n",
    "\n",
    "The crew will:\n",
    "1. **Generate suggestions** for improvements based on support data\n",
    "2. **Create tables** summarizing key metrics and trends\n",
    "3. **Plot charts** to visualize trends and patterns\n",
    "4. **Assemble a final report** bringing everything together\n",
    "\n",
    "We'll use agents that can execute code in a Docker sandbox for security."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "support-analysis-yaml-loading",
   "metadata": {},
   "source": [
    "### Loading Configuration Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "support-analysis-yaml",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YAML configurations for support data analysis\n",
    "files = {\n",
    "    'agents': r\"agents\\data insight agents.yaml\",\n",
    "    'tasks': r\"tasks\\data insight tasks.yaml\"\n",
    "}\n",
    "\n",
    "configs = load_yaml_config(files)\n",
    "agents_config = configs['agents']\n",
    "tasks_config = configs['tasks']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "support-analysis-crew-creation",
   "metadata": {},
   "source": [
    "### Creating Agents, Tasks, and Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046cec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai.tools import BaseTool\n",
    "from pydantic import BaseModel, Field\n",
    "from crewai_tools import FileReadTool\n",
    "from typing import Type\n",
    "\n",
    "class FileReadInput(BaseModel):\n",
    "    file_path: str = Field(..., description=\"Path to the file to read\")\n",
    "\n",
    "class FileReadWrapper(BaseTool):\n",
    "    name: str = \"file_reader\"\n",
    "    description: str = \"Reads text data from a specified file\"\n",
    "    args_schema: Type[BaseModel] = FileReadInput  # Properly annotated\n",
    "\n",
    "    def _run(self, file_path: str) -> str:\n",
    "        tool = FileReadTool(file_path=\"support tickets data.csv\")\n",
    "        return tool.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "support-analysis-crew",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_tool = FileReadWrapper()\n",
    "\n",
    "# Create agents with specific capabilities\n",
    "suggestion_generation_agent = Agent(\n",
    "    config=agents_config['suggestion_generation_agent'],\n",
    "    tools=[csv_tool]  # Access to support data\n",
    ")\n",
    "\n",
    "reporting_agent = Agent(\n",
    "    config=agents_config['reporting_agent'],\n",
    "    tools=[csv_tool]  # Access to support data\n",
    ")\n",
    "\n",
    "chart_generation_agent = Agent(\n",
    "    config=agents_config['chart_generation_agent'],\n",
    "    allow_code_execution=False\n",
    "    #If you have docker installed in your machine you can set\n",
    "    #allow_code_execution=True  # KEY: Enables code execution in Docker sandbox\n",
    ")\n",
    "\n",
    "# Create tasks with dependencies\n",
    "suggestion_generation = Task(\n",
    "    config=tasks_config['suggestion_generation'],\n",
    "    agent=suggestion_generation_agent\n",
    ")\n",
    "\n",
    "table_generation = Task(\n",
    "    config=tasks_config['table_generation'],\n",
    "    agent=reporting_agent\n",
    ")\n",
    "\n",
    "chart_generation = Task(\n",
    "    config=tasks_config['chart_generation'],\n",
    "    agent=chart_generation_agent\n",
    ")\n",
    "\n",
    "final_report_assembly = Task(\n",
    "    config=tasks_config['final_report_assembly'],\n",
    "    agent=reporting_agent,\n",
    "    context=[suggestion_generation, table_generation, chart_generation]  # Uses outputs from all previous tasks\n",
    ")\n",
    "\n",
    "# Create the crew\n",
    "support_crew = Crew(\n",
    "    agents=[suggestion_generation_agent, reporting_agent, chart_generation_agent],\n",
    "    tasks=[suggestion_generation, table_generation, chart_generation, final_report_assembly],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3335b1b9",
   "metadata": {},
   "source": [
    "### Testing our Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26dd5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "support_crew.test(n_iterations=1, eval_llm='gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e240b588",
   "metadata": {},
   "source": [
    "### Training your crew and agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_crew.train(n_iterations=1, filename='training.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb00fe1f",
   "metadata": {},
   "source": [
    "### Comparing new test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f2b307",
   "metadata": {},
   "outputs": [],
   "source": [
    "support_crew.test(n_iterations=1, eval_llm='gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2ec195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Trello screenshot\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Load and display the image\n",
    "test_image = Image(filename='images/test before training.png', width=368)\n",
    "display(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "support-analysis-execution",
   "metadata": {},
   "source": [
    "### Kicking off Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "support-analysis-execution-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the support data analysis crew\n",
    "support_result = support_crew.kickoff()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "support-analysis-results",
   "metadata": {},
   "source": [
    "### Results and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46c4a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "support_crew.usage_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "support-analysis-results-display",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate costs for the analysis\n",
    "total_tokens = support_crew.usage_metrics.prompt_tokens + support_crew.usage_metrics.completion_tokens\n",
    "total_cost = 0.150 * total_tokens / 1_000_000\n",
    "display(f\"Support analysis costs: ${total_cost:.4f}\")\n",
    "\n",
    "# Display the comprehensive report with charts and tables\n",
    "display(Markdown(support_result.raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-10-header",
   "metadata": {},
   "source": [
    "## Section 10: Multi Model Use Cases\n",
    "\n",
    "A multi-model approach lets you mix and match different LLM models based on the task at hand, creating more efficient and customized AI systems.\n",
    "\n",
    "### Model Selection Strategy:\n",
    "\n",
    "**Smaller, Faster Models**: Use for simple tasks like data extraction, basic research, or initial processing\n",
    "**Larger, More Capable Models**: Use for complex reasoning, content generation, or tasks requiring nuanced understanding\n",
    "**Fine-tuned Models**: Use for domain-specific tasks or when you need to match specific brand voice/style\n",
    "\n",
    "### Implementation Approaches:\n",
    "\n",
    "1. **Agent-Level Model Assignment**: Different agents use different models based on their role\n",
    "2. **Task-Level Model Assignment**: Same agent uses different models for different tasks\n",
    "3. **Dynamic Model Selection**: Model choice based on task complexity or other runtime factors\n",
    "\n",
    "### Benefits:\n",
    "- **Cost Optimization**: Use expensive models only when necessary\n",
    "- **Speed Optimization**: Use fast models for time-sensitive tasks\n",
    "- **Quality Optimization**: Use best models for critical tasks\n",
    "- **Specialization**: Use domain-specific models where appropriate\n",
    "\n",
    "The key is finding the right balance between speed, cost, and quality for your specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-11-header",
   "metadata": {},
   "source": [
    "## Section 11: Content Creation at Scale\n",
    "\n",
    "This use case builds a crew for content creation at scale, monitoring the web and using RAG (Retrieval Augmented Generation) to create amazing blog posts and social media content.\n",
    "\n",
    "We'll use multiple models for optimization:\n",
    "- **Groq with Llama 3.1-70B**: For speed-optimized tasks\n",
    "- **GPT-4o-mini**: For balanced performance\n",
    "\n",
    "The crew will:\n",
    "1. **Monitor latest news** around financial topics\n",
    "2. **Analyze market data** and trends\n",
    "3. **Create content** for blogs and social media\n",
    "4. **Review content** for quality assurance\n",
    "\n",
    "We'll use RAG as a tool - agents automatically download content, chunk it, embed it, and save to a vector database for real-time search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "content-creation-pydantic",
   "metadata": {},
   "source": [
    "### Structured Output Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "content-creation-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models for content creation output\n",
    "class SocialMediaPost(BaseModel):\n",
    "    platform: str = Field(..., description=\"Social media platform (LinkedIn, Twitter, Instagram, etc.)\")\n",
    "    content: str = Field(..., description=\"Content for the social media post\")\n",
    "\n",
    "class ContentOutput(BaseModel):\n",
    "    article: str = Field(..., description=\"Full blog article content\")\n",
    "    social_media_posts: List[SocialMediaPost] = Field(..., description=\"List of social media posts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "content-creation-yaml-loading",
   "metadata": {},
   "source": [
    "### Loading Configuration Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "content-creation-yaml",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load content creation configurations\n",
    "files = {\n",
    "    'agents': r'agents\\content creation agents.yaml',\n",
    "    'tasks': r'tasks\\content creation tasks.yaml'\n",
    "}\n",
    "\n",
    "configs = load_yaml_config(files)\n",
    "agents_config = configs['agents']\n",
    "tasks_config = configs['tasks']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "content-creation-tools",
   "metadata": {},
   "source": [
    "### Tool Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6b0a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai.tools.base_tool import BaseTool\n",
    "from crewai_tools import SerperDevTool, ScrapeWebsiteTool, WebsiteSearchTool\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "\n",
    "class SearchToolSchema(BaseModel):\n",
    "    query: str\n",
    "\n",
    "class ScrapeToolInput(BaseModel):\n",
    "    url: str\n",
    "\n",
    "class SerperWrapper(BaseTool):\n",
    "    name: str = \"serper_search\"\n",
    "    description: str = \"Performs web search using Serper\"\n",
    "    args_schema: type = SearchToolSchema\n",
    "\n",
    "    def _run(self, query: str) -> str:\n",
    "        tool = SerperDevTool()\n",
    "        return tool.run(query)\n",
    "\n",
    "class ScrapeWrapper(BaseTool):\n",
    "    name: str = \"web_scraper\"\n",
    "    description: str = \"Scrapes a specified website for content\"\n",
    "    args_schema: type = ScrapeToolInput\n",
    "\n",
    "    def _run(self, url: str) -> str:\n",
    "        tool = ScrapeWebsiteTool(website_url=url)\n",
    "        return tool.run()\n",
    "    \n",
    "class WebsiteSearchInput(BaseModel):\n",
    "    search_query: str = Field(..., description=\"Search query to perform\")\n",
    "    website: Optional[str] = Field(None, description=\"Optional URL to limit the search\")\n",
    "\n",
    "class WebsiteSearchWrapper(BaseTool):\n",
    "    name: str = \"website_search\"\n",
    "    description: str = \"Performs semantic search across website content\"\n",
    "    args_schema: Type[BaseModel] = WebsiteSearchInput\n",
    "\n",
    "    def _run(self, search_query: str, website: Optional[str] = None) -> str:\n",
    "        if website:\n",
    "            tool = WebsiteSearchTool(website=website)\n",
    "        else:\n",
    "            tool = WebsiteSearchTool()\n",
    "        return tool.run(search_query=search_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "content-creation-tools-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internet search tool\n",
    "search_tool = SerperWrapper()\n",
    "\n",
    "# Website scraping tool\n",
    "scrape_tool = ScrapeWrapper()\n",
    "\n",
    "# RAG as a tool - automatically creates vector database from web content\n",
    "website_rag = WebsiteSearchWrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "content-creation-crew-creation",
   "metadata": {},
   "source": [
    "### Creating Agents, Tasks, and Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "content-creation-crew",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import LLM\n",
    "\n",
    "# Create LLM instances\n",
    "openai_llm = LLM(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Creating Agents\n",
    "market_news_monitor_agent = Agent(\n",
    "    config=agents_config['market_news_monitor_agent'],\n",
    "    tools=[search_tool, website_rag],\n",
    "    llm=openai_llm,\n",
    ")\n",
    "\n",
    "data_analyst_agent = Agent(\n",
    "    config=agents_config['data_analyst_agent'],\n",
    "    tools=[search_tool, website_rag],\n",
    "    llm=openai_llm,\n",
    ")\n",
    "\n",
    "content_creator_agent = Agent(\n",
    "    config=agents_config['content_creator_agent'],\n",
    "    tools=[search_tool, website_rag],\n",
    ")\n",
    "\n",
    "quality_assurance_agent = Agent(\n",
    "    config=agents_config['quality_assurance_agent'],\n",
    ")\n",
    "\n",
    "# Creating Tasks\n",
    "monitor_financial_news_task = Task(\n",
    "    config=tasks_config['monitor_financial_news'],\n",
    "    agent=market_news_monitor_agent\n",
    ")\n",
    "\n",
    "analyze_market_data_task = Task(\n",
    "    config=tasks_config['analyze_market_data'],\n",
    "    agent=data_analyst_agent\n",
    ")\n",
    "\n",
    "create_content_task = Task(\n",
    "    config=tasks_config['create_content'],\n",
    "    agent=content_creator_agent,\n",
    "    context=[monitor_financial_news_task, analyze_market_data_task]\n",
    ")\n",
    "\n",
    "quality_assurance_task = Task(\n",
    "    config=tasks_config['quality_assurance'],\n",
    "    agent=quality_assurance_agent,\n",
    "    output_pydantic=ContentOutput\n",
    ")\n",
    "\n",
    "# Create the content creation crew\n",
    "content_crew = Crew(\n",
    "    agents=[\n",
    "        market_news_monitor_agent,\n",
    "        data_analyst_agent,\n",
    "        content_creator_agent,\n",
    "        quality_assurance_agent\n",
    "    ],\n",
    "    tasks=[\n",
    "        monitor_financial_news_task,\n",
    "        analyze_market_data_task,\n",
    "        create_content_task,\n",
    "        quality_assurance_task\n",
    "    ],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "content-creation-execution",
   "metadata": {},
   "source": [
    "### Executing Content Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "content-creation-execution-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = content_crew.kickoff(inputs={\n",
    "  'subject': 'Inflation in the US and the impact on the stock market in 2024'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9780df8c",
   "metadata": {},
   "source": [
    "### Social Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cceaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = result.pydantic.dict()['social_media_posts']\n",
    "for post in posts:\n",
    "    platform = post['platform']\n",
    "    content = post['content']\n",
    "    print(platform)\n",
    "    wrapped_content = textwrap.fill(content, width=50)\n",
    "    print(wrapped_content)\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "content-creation-results",
   "metadata": {},
   "source": [
    "### Blog Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "content-creation-results-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(result.pydantic.dict()['article']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-12-header",
   "metadata": {},
   "source": [
    "## Section 12: Agentic Workflows in Industry\n",
    "\n",
    "Real-world deployment of agentic workflows involves several considerations beyond just building the crews.\n",
    "\n",
    "### Industry Applications:\n",
    "\n",
    "**Sales**: Lead qualification, personalized outreach, proposal generation\n",
    "**Marketing**: Content creation, campaign optimization, competitor analysis\n",
    "**HR**: Resume screening, interview scheduling, onboarding automation\n",
    "**Support**: Ticket triage, knowledge base updates, escalation management\n",
    "**Operations**: Process optimization, report generation, compliance monitoring\n",
    "\n",
    "### Deployment Patterns:\n",
    "\n",
    "**Batch Processing**: Run crews on schedules to process accumulated data\n",
    "**Real-time Processing**: Trigger crews based on events or API calls\n",
    "**Human-in-the-Loop**: Include human approval steps for critical decisions\n",
    "**Escalation Handling**: Route complex cases to human experts\n",
    "\n",
    "### Scalability Considerations:\n",
    "\n",
    "- **Error Handling**: Robust error handling and recovery mechanisms\n",
    "- **Rate Limiting**: Manage API usage and costs\n",
    "- **Monitoring**: Track performance, costs, and quality metrics\n",
    "- **Versioning**: Manage changes to agents and tasks over time\n",
    "- **Security**: Protect sensitive data and API credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-13-header",
   "metadata": {},
   "source": [
    "## Section 13: Generate, Deploy and Monitor Crews\n",
    "\n",
    "Let's learn how to generate, deploy, and monitor crews in production environments.\n",
    "\n",
    "### CrewAI CLI:\n",
    "\n",
    "Start a new crew project with a single command:\n",
    "```bash\n",
    "crewai create crew project_name\n",
    "```\n",
    "\n",
    "This creates the complete folder structure:\n",
    "- **README.md**: Instructions and dependencies\n",
    "- **agents.yaml** and **tasks.yaml**: Configuration files\n",
    "- **tools/**: Custom tools directory\n",
    "- **crew.py**: Main crew definition\n",
    "- **main.py**: Entry point for local execution\n",
    "\n",
    "### Flow Creation:\n",
    "\n",
    "Create flows for multi-crew orchestration:\n",
    "```bash\n",
    "crewai create flow flow_name\n",
    "```\n",
    "\n",
    "Flows have a **crews/** folder for multiple crews and orchestrate them in **main.py**.\n",
    "\n",
    "### Running Crews:\n",
    "\n",
    "Execute your crew locally:\n",
    "```bash\n",
    "crewai run\n",
    "```\n",
    "\n",
    "### Environment Setup:\n",
    "\n",
    "- **.env file**: Store API keys and configuration\n",
    "- **requirements.txt**: Python dependencies\n",
    "- **Virtual environments**: Isolate dependencies\n",
    "\n",
    "This structure makes it easy to version control, share, and deploy your crews professionally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-14-header",
   "metadata": {},
   "source": [
    "## Section 14: Blog Post Crew in Production\n",
    "\n",
    "When you create a crew from scratch using the CLI, it comes with a basic blog creation example to help you understand the structure.\n",
    "\n",
    "### Default Crew Structure:\n",
    "\n",
    "The generated crew includes:\n",
    "- **Senior Data Researcher**: Researches content topics\n",
    "- **Blog Content Writer**: Writes articles based on research\n",
    "\n",
    "This simple two-agent crew demonstrates the research → writing pattern that's common in many use cases.\n",
    "\n",
    "### Production Considerations:\n",
    "\n",
    "**Environment Variables**: All API keys and configuration should be in environment variables, not hardcoded\n",
    "**Error Handling**: Production crews need robust error handling for network issues, API failures, etc.\n",
    "**Logging**: Comprehensive logging for debugging and monitoring\n",
    "**Testing**: Unit tests for individual components and integration tests for full crews\n",
    "**Monitoring**: Track execution times, costs, and success rates\n",
    "**Scaling**: Consider parallel execution and resource management for high-volume scenarios\n",
    "\n",
    "### Deployment Options:\n",
    "\n",
    "- **Local Deployment**: Run on your own servers\n",
    "- **Cloud Functions**: Serverless execution for event-driven workflows\n",
    "- **Container Orchestration**: Docker containers with Kubernetes\n",
    "- **CI/CD Integration**: Automated testing and deployment pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-15-header",
   "metadata": {},
   "source": [
    "## Section 15: Conclusion\n",
    "\n",
    "You have completed this comprehensive practical course on Multi-AI Agent Systems with CrewAI.\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "1. **Foundation**: Understanding of multi-agent systems, their anatomy, and building blocks\n",
    "2. **Practical Applications**: Built real-world use cases including:\n",
    "   - Automated project planning and estimation\n",
    "   - Project progress reporting with external integrations\n",
    "   - Agentic sales pipelines with Flows\n",
    "   - Support data analysis with code-executing agents\n",
    "   - Content creation at scale with RAG and multi-model approaches\n",
    "\n",
    "3. **Advanced Features**: \n",
    "   - External system integrations\n",
    "   - Complex crew orchestration patterns\n",
    "   - Performance optimization through testing and training\n",
    "   - Multi-model strategies for cost and speed optimization\n",
    "\n",
    "4. **Production Deployment**: \n",
    "   - CLI tools for project generation\n",
    "   - Production considerations and best practices\n",
    "   - Monitoring and maintenance strategies\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- **Start Simple**: Begin with basic sequential crews and add complexity as needed\n",
    "- **Test and Train**: Use CrewAI's built-in testing and training features for continuous improvement\n",
    "- **Optimize Strategically**: Balance speed, quality, and cost based on your specific requirements\n",
    "- **Think Production**: Consider error handling, monitoring, and scalability from the beginning\n",
    "- **Iterate and Improve**: Use human feedback to continuously refine your agents and tasks\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Experiment**: Try building crews for your own use cases\n",
    "2. **Deploy**: Move from notebook experiments to production systems\n",
    "3. **Monitor**: Track performance and continuously improve\n",
    "4. **Scale**: Apply these patterns to larger, more complex problems\n",
    "5. **Share**: Contribute to the CrewAI community with your innovations\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- **CrewAI Documentation**: Latest features and best practices\n",
    "- **Community Forums**: Connect with other practitioners\n",
    "- **GitHub Repository**: Access examples and contribute improvements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fc9d7c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
